{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Data Science Coding Challange!¶\n",
    "Test your skills in a real-world coding challenge. Coding Challenges provide CS & DS Coding Competitions with Prizes and achievement badges!\n",
    "\n",
    "CS & DS learners want to be challenged as a way to evaluate if they’re job ready. So, why not create fun challenges and give winners something truly valuable such as complimentary access to select Data Science courses, or the ability to receive an achievement badge on their Coursera Skills Profile - highlighting their performance to recruiters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this challenge, you'll get the opportunity to tackle one of the most industry-relevant maching learning problems with a unique dataset that will put your modeling skills to the test. Subscription services are leveraged by companies across many industries, from fitness to video streaming to retail. One of the primary objectives of companies with subscription services is to decrease churn and ensure that users are retained as subscribers. In order to do this efficiently and systematically, many companies employ machine learning to predict which users are at the highest risk of churn, so that proper interventions can be effectively deployed to the right audience.\n",
    "\n",
    "In this challenge, we will be tackling the churn prediction problem on a very unique and interesting group of subscribers on a video streaming service!\n",
    "\n",
    "Imagine that you are a new data scientist at this video streaming company and you are tasked with building a model that can predict which existing subscribers will continue their subscriptions for another month. We have provided a dataset that is a sample of subscriptions that were initiated in 2021, all snapshotted at a particular date before the subscription was cancelled. Subscription cancellation can happen for a multitude of reasons, including:\n",
    "\n",
    "the customer completes all content they were interested in, and no longer need the subscription\n",
    "the customer finds themselves to be too busy and cancels their subscription until a later time\n",
    "the customer determines that the streaming service is not the best fit for them, so they cancel and look for something better suited\n",
    "Regardless the reason, this video streaming company has a vested interest in understanding the likelihood of each individual customer to churn in their subscription so that resources can be allocated appropriately to support customers. In this challenge, you will use your machine learning toolkit to do just that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Datasets\n",
    "Train vs. Test\n",
    "In this competition, you’ll gain access to two datasets that are samples of past subscriptions of a video streaming platform that contain information about the customer, the customers streaming preferences, and their activity in the subscription thus far. One dataset is titled train.csv and the other is titled test.csv.\n",
    "\n",
    "train.csv contains 70% of the overall sample (243,787 subscriptions to be exact) and importantly, will reveal whether or not the subscription was continued into the next month (the “ground truth”).\n",
    "\n",
    "The test.csv dataset contains the exact same information about the remaining segment of the overall sample (104,480 subscriptions to be exact), but does not disclose the “ground truth” for each subscription. It’s your job to predict this outcome!\n",
    "\n",
    "Using the patterns you find in the train.csv data, predict whether the subscriptions in test.csv will be continued for another month, or not.\n",
    "\n",
    "Dataset descriptions\n",
    "Both train.csv and test.csv contain one row for each unique subscription. For each subscription, a single observation (CustomerID) is included during which the subscription was active.\n",
    "\n",
    "In addition to this identifier column, the train.csv dataset also contains the target label for the task, a binary column Churn.\n",
    "\n",
    "Besides that column, both datasets have an identical set of features that can be used to train your model to make predictions. Below you can see descriptions of each feature. Familiarize yourself with them so that you can harness them most effectively for this machine learning task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_descriptions = pd.read_csv('data_descriptions.csv')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "data_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "\n",
    "# Data packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning / Classification packages\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Visualization Packages\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing other packages which will be used\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "Let's start by loading the dataset train.csv into a dataframe train_df, and test.csv into a dataframe test_df and display the shape of the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "print('train_df Shape:', train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "print('test_df Shape:', test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring, Cleaning, Validating, and Visualizing the Data \n",
    "Here we need to explore, clean, validate, and visualize the data however you see fit for this competition to help determine or optimize your predictive model. Please note - the final autograding will only be on the accuracy of the prediction_df predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Churn'].value_counts().plot(kind='bar')\n",
    "\n",
    "plt.title('Churn distribution using Training Data')\n",
    "plt.xlabel('Churn')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML MODEL\n",
    "### Using XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your system is not having XGBoost install using command\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(['CustomerID','Churn'], axis=1)\n",
    "y_train = train_df['Churn']\n",
    "X_test = test_df.drop(['CustomerID'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage Patterns\n",
    "X_train['AverageViewingDurationPerWeek'] = X_train['AverageViewingDuration'] / (X_train['ViewingHoursPerWeek'] + 1e-8)\n",
    "X_test['AverageViewingDurationPerWeek'] = X_test['AverageViewingDuration'] / (X_test['ViewingHoursPerWeek'] + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated Features\n",
    "X_train['AverageViewingDurationBySubscriptionType'] = X_train.groupby('SubscriptionType')['AverageViewingDuration'].transform('mean')\n",
    "X_test['AverageViewingDurationBySubscriptionType'] = X_test.groupby('SubscriptionType')['AverageViewingDuration'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction Ratios\n",
    "X_train['ContentDownloadsToViewingHoursRatio'] = X_train['ContentDownloadsPerMonth'] / (X_train['ViewingHoursPerWeek'] * 4 + 1e-8)\n",
    "X_test['ContentDownloadsToViewingHoursRatio'] = X_test['ContentDownloadsPerMonth'] / (X_test['ViewingHoursPerWeek'] * 4 + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = X_train.select_dtypes(include=['object', 'string']).columns\n",
    "for col in categorical_columns:\n",
    "    X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "    X_test[col] = label_encoder.transform(X_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(n_estimators=100, random_state=0)\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probability = 1-xgb_model.predict_proba(X_test)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine predictions with label column into a dataframe\n",
    "prediction_df = pd.DataFrame({'CustomerID': test_df[['CustomerID']].values[:, 0],\n",
    "                              'predicted_probability': predicted_probability})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensuring it should contain 104,480 rows and 2 columns 'CustomerID' and 'predicted_probaility'\n",
    "print(prediction_df.shape)\n",
    "prediction_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to csv for autograding purposes\n",
    "prediction_df.to_csv(\"prediction_submission.csv\", index=False)\n",
    "submission = pd.read_csv(\"prediction_submission.csv\")\n",
    "\n",
    "assert isinstance(submission, pd.DataFrame), 'You should have a dataframe named prediction_df.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert submission.columns[0] == 'CustomerID', 'The first column name should be CustomerID.'\n",
    "assert submission.columns[1] == 'predicted_probability', 'The second column name should be predicted_probability.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert submission.shape[0] == 104480, 'The dataframe prediction_df should have 104480 rows.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert submission.shape[1] == 2, 'The dataframe prediction_df should have 2 columns.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Submitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
